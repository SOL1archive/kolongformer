{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aad_1s7ybD5o"
      },
      "source": [
        "# `RoBERTa` --> `Longformer`: build a \"long\" version of pretrained models\n",
        "\n",
        "This notebook replicates the procedure descriped in the [Longformer paper](https://arxiv.org/abs/2004.05150) to train a Longformer model starting from the RoBERTa checkpoint. The same procedure can be applied to build the \"long\" version of other pretrained models as well.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BieXv0YUd7NF"
      },
      "source": [
        "### Data, libraries, and imports\n",
        "Our procedure requires a corpus for pretraining. For demonstration, we will use Wikitext103; a corpus of 100M tokens from wikipedia articles. Depending on your application, consider using a different corpus that is a better match."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AZZ4VmKSwvH"
      },
      "outputs": [],
      "source": [
        "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
        "!unzip wikitext-103-raw-v1.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "o3yjIYKXw3rL"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate -U --quiet\n",
        "!pip install datasets -U --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "U0NnMMl6wy7Q"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import math\n",
        "import copy\n",
        "import pathlib\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import RobertaTokenizerFast, DataCollatorForLanguageModeling, Trainer, AutoTokenizer\n",
        "from transformers import RobertaForMaskedLM, AutoModel, AutoModelForMaskedLM, LongformerForMaskedLM\n",
        "from transformers import TrainingArguments, HfArgumentParser\n",
        "from transformers import LongformerSelfAttention\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(level=logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYn4WZ7rACVv",
        "outputId": "a02928d9-86d6-45c2-c197-c8c80664ed54"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
      ],
      "metadata": {
        "id": "HLsUAkDYAESP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_path = '/content/drive/MyDrive/datasets/KorCrawl'\n",
        "main_path = Path(main_path)"
      ],
      "metadata": {
        "id": "k4vXHdqAAGLD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgoNVJYUbD59"
      },
      "source": [
        "### RobertaLong\n",
        "\n",
        "`RobertaLongForMaskedLM` represents the \"long\" version of the `RoBERTa` model. It replaces `BertSelfAttention` with `RobertaLongSelfAttention`, which is a thin wrapper around `LongformerSelfAttention`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "J9EBISkRxPjO"
      },
      "outputs": [],
      "source": [
        "class RobertaLongSelfAttention(LongformerSelfAttention):\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        return super().forward(hidden_states, attention_mask=attention_mask, output_attentions=output_attentions)\n",
        "\n",
        "\n",
        "class RobertaLongForMaskedLM(RobertaForMaskedLM):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        for i, layer in enumerate(self.roberta.encoder.layer):\n",
        "            # replace the `modeling_bert.BertSelfAttention` object with `LongformerSelfAttention`\n",
        "            layer.attention.self = RobertaLongSelfAttention(config, layer_id=i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LRZa5s1bD6E"
      },
      "source": [
        "Starting from the `roberta-base` checkpoint, the following function converts it into an instance of `RobertaLong`. It makes the following changes:\n",
        "\n",
        "- extend the position embeddings from `512` positions to `max_pos`. In Longformer, we set `max_pos=4096`\n",
        "\n",
        "- initialize the additional position embeddings by copying the embeddings of the first `512` positions. This initialization is crucial for the model performance (check table 6 in [the paper](https://arxiv.org/pdf/2004.05150.pdf) for performance without this initialization)\n",
        "\n",
        "- replaces `modeling_bert.BertSelfAttention` objects with `modeling_longformer.LongformerSelfAttention` with a attention window size `attention_window`\n",
        "\n",
        "The output of this function works for long documents even without pretraining. Check tables 6 and 11 in [the paper](https://arxiv.org/pdf/2004.05150.pdf) to get a sense of the expected performance of this model before pretraining."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "-m4A_ttixPuf"
      },
      "outputs": [],
      "source": [
        "def create_long_model(save_model_to, attention_window, max_pos):\n",
        "    model = RobertaForMaskedLM.from_pretrained('klue/roberta-base')\n",
        "    tokenizer = RobertaTokenizerFast.from_pretrained('klue/roberta-base', model_max_length=max_pos)\n",
        "    config = model.config\n",
        "\n",
        "    # extend position embeddings\n",
        "    tokenizer.model_max_length = max_pos\n",
        "    tokenizer.init_kwargs['model_max_length'] = max_pos\n",
        "    current_max_pos, embed_size = model.roberta.embeddings.position_embeddings.weight.shape\n",
        "    max_pos += 2  # NOTE: RoBERTa has positions 0,1 reserved, so embedding size is max position + 2\n",
        "    config.max_position_embeddings = max_pos\n",
        "    assert max_pos > current_max_pos\n",
        "    # allocate a larger position embedding matrix\n",
        "    new_pos_embed = model.roberta.embeddings.position_embeddings.weight.new_empty(max_pos, embed_size)\n",
        "    # copy position embeddings over and over to initialize the new position embeddings\n",
        "    k = 2\n",
        "    step = current_max_pos - 2\n",
        "    while k < max_pos - 1:\n",
        "        new_pos_embed[k:(k + step)] = model.roberta.embeddings.position_embeddings.weight[2:]\n",
        "        k += step\n",
        "    model.roberta.embeddings.position_embeddings.weight.data = new_pos_embed\n",
        "    model.roberta.embeddings.position_ids.data = torch.tensor([i for i in range(max_pos)]).reshape(1, max_pos)\n",
        "\n",
        "    # replace the `modeling_bert.BertSelfAttention` object with `LongformerSelfAttention`\n",
        "    config.attention_window = [attention_window] * config.num_hidden_layers\n",
        "    for i, layer in enumerate(model.roberta.encoder.layer):\n",
        "        longformer_self_attn = LongformerSelfAttention(config, layer_id=i)\n",
        "        longformer_self_attn.query = layer.attention.self.query\n",
        "        longformer_self_attn.key = layer.attention.self.key\n",
        "        longformer_self_attn.value = layer.attention.self.value\n",
        "\n",
        "        longformer_self_attn.query_global = copy.deepcopy(layer.attention.self.query)\n",
        "        longformer_self_attn.key_global = copy.deepcopy(layer.attention.self.key)\n",
        "        longformer_self_attn.value_global = copy.deepcopy(layer.attention.self.value)\n",
        "\n",
        "        layer.attention.self = longformer_self_attn\n",
        "\n",
        "    logger.info(f'saving model to {save_model_to}')\n",
        "    model.save_pretrained(save_model_to)\n",
        "    tokenizer.save_pretrained(save_model_to)\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkbQvhOMbD6L"
      },
      "source": [
        "Pretraining on Masked Language Modeling (MLM) doesn't update the global projection layers. After pretraining, the following function copies `query`, `key`, `value` to their global counterpart projection matrices.\n",
        "For more explanation on \"local\" vs. \"global\" attention, please refer to the documentation [here](https://huggingface.co/transformers/model_doc/longformer.html#longformer-self-attention)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "CO3MoEgCxP9W"
      },
      "outputs": [],
      "source": [
        "def copy_proj_layers(model):\n",
        "    for i, layer in enumerate(model.roberta.encoder.layer):\n",
        "        layer.attention.self.query_global = copy.deepcopy(layer.attention.self.query)\n",
        "        layer.attention.self.key_global = copy.deepcopy(layer.attention.self.key)\n",
        "        layer.attention.self.value_global = copy.deepcopy(layer.attention.self.value)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KjI9f8BbD6S"
      },
      "source": [
        "### Pretrain and Evaluate on masked language modeling (MLM)\n",
        "\n",
        "The following function pretrains and evaluates a model on MLM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqY_Hg5HbD6a"
      },
      "source": [
        "**Training hyperparameters**\n",
        "\n",
        "- Following RoBERTa pretraining setting, we set number of tokens per batch to be `2^18` tokens. Changing this number might require changes in the lr, lr-scheudler, #steps and #warmup steps. Therefor, it is a good idea to keep this number constant.\n",
        "\n",
        "- Note that: `#tokens/batch = batch_size x #gpus x gradient_accumulation x seqlen`\n",
        "   \n",
        "- In [the paper](https://arxiv.org/pdf/2004.05150.pdf), we train for 65k steps, but 3k is probably enough (check table 6)\n",
        "\n",
        "- **Important note**: The lr-scheduler in [the paper](https://arxiv.org/pdf/2004.05150.pdf) is polynomial_decay with power 3 over 65k steps. To train for 3k steps, use a constant lr-scheduler (after warmup). Both lr-scheduler are not supported in HF trainer, and at least **constant lr-scheduler** will need to be added.\n",
        "\n",
        "- Pretraining will take 2 days on 1 x 32GB GPU with fp32. Consider using fp16 and using more gpus to train faster (if you increase `#gpus`, reduce `gradient_accumulation` to maintain `#tokens/batch` as mentioned earlier).\n",
        "\n",
        "- As a demonstration, this notebook is training on wikitext103 but wikitext103 is rather small that it takes 7 epochs to train for 3k steps Consider doing a single epoch on a larger dataset (800M tokens) instead.\n",
        "\n",
        "- Set #gpus using `CUDA_VISIBLE_DEVICES`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        },
        "id": "Zl_hDDlryVo2",
        "outputId": "d9b779c4-9f0c-4926-c61d-2f0105e5f8b4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-126-4facce947c97>, line 27)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-126-4facce947c97>\"\u001b[0;36m, line \u001b[0;32m27\u001b[0m\n\u001b[0;31m    training_args.\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "@dataclass\n",
        "class ModelArgs:\n",
        "    attention_window: int = field(default=512, metadata={\"help\": \"Size of attention window\"})\n",
        "    max_pos: int = field(default=4096, metadata={\"help\": \"Maximum position\"})\n",
        "\n",
        "parser = HfArgumentParser((TrainingArguments, ModelArgs,))\n",
        "\n",
        "training_args, model_args = parser.parse_args_into_dataclasses(look_for_args_file=False, args=[\n",
        "    '--output_dir', '/content/drive/MyDrive/projects/ExpertLLM/model/',\n",
        "    '--warmup_steps', '500',\n",
        "    '--learning_rate', '0.00003',\n",
        "    '--weight_decay', '0.01',\n",
        "    '--adam_epsilon', '1e-6',\n",
        "    '--max_steps', '3000',\n",
        "    '--logging_steps', '500',\n",
        "    #'--save_steps', '500',\n",
        "    '--max_grad_norm', '5.0',\n",
        "    '--per_device_eval_batch_size', '8',\n",
        "    '--per_device_train_batch_size', '2',  # 32GB gpu with fp32\n",
        "    '--gradient_accumulation_steps', '32',\n",
        "    #'--evaluate_during_training',\n",
        "    '--do_train',\n",
        "    '--do_eval',\n",
        "])\n",
        "training_args.val_datapath = main_path / 'sample_train.csv'\n",
        "training_args.train_datapath = main_path / 'sample_val.csv'\n",
        "\n",
        "# Choose GPU\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi5ZIKcsbD6e"
      },
      "source": [
        "### Put it all together\n",
        "\n",
        "1) Evaluating `roberta-base` on MLM to establish a baseline. Validation `bpc` = `2.536` which is higher than the `bpc` values in table 6 [here](https://arxiv.org/pdf/2004.05150.pdf) because wikitext103 is harder than our pretraining corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiKj7D1c1ovy",
        "outputId": "55b45726-3e5e-49a5-f044-0a5f92b3bf3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
            "The class this function is called from is 'RobertaTokenizerFast'.\n"
          ]
        }
      ],
      "source": [
        "roberta_base = RobertaForMaskedLM.from_pretrained('klue/roberta-base')\n",
        "roberta_base_tokenizer = RobertaTokenizerFast.from_pretrained('klue/roberta-base')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBXU3r69bD6l"
      },
      "source": [
        "2) As descriped in `create_long_model`, convert a `roberta-base` model into `roberta-base-4096` which is an instance of `RobertaLong`, then save it to the disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7tcsSfZ1-b9",
        "outputId": "90283d4f-2f55-427f-a198-450ed151b0f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
            "The class this function is called from is 'RobertaTokenizerFast'.\n"
          ]
        }
      ],
      "source": [
        "model_path = f'{training_args.output_dir}/roberta-base-{model_args.max_pos}'\n",
        "if not os.path.exists(model_path):\n",
        "    os.makedirs(model_path)\n",
        "\n",
        "logger.info(f'Converting roberta-base into roberta-base-{model_args.max_pos}')\n",
        "model, tokenizer = create_long_model(\n",
        "    save_model_to=model_path, attention_window=model_args.attention_window, max_pos=model_args.max_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiftMH3-zPUS"
      },
      "source": [
        "3) Load `roberta-base-4096` from the disk. This model works for long sequences even without pretraining. If you don't want to pretrain, you can stop here and start finetuning your `roberta-base-4096` on downstream tasks 🎉🎉🎉"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "H8vNeYdrzMd2"
      },
      "outputs": [],
      "source": [
        "logger.info(f'Loading the model from {model_path}')\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(model_path)\n",
        "model = RobertaLongForMaskedLM.from_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS0Np2F4bD6p"
      },
      "source": [
        "4) Pretrain `roberta-base-4096` for `3k` steps, each steps has `2^18` tokens. Notes:\n",
        "\n",
        "- The `training_args.max_steps = 3 ` is just for the demo. **Remove this line for the actual training**\n",
        "\n",
        "- Training for `3k` steps will take 2 days on a single 32GB gpu with `fp32`. Consider using `fp16` and more gpus to train faster.\n",
        "\n",
        "- Tokenizing the training data the first time is going to take 5-10 minutes.\n",
        "\n",
        "- MLM validation `bpc` **before** pretraining: **2.652**, a bit worse than the **2.536** of `roberta-base`. As discussed in [the paper](https://arxiv.org/pdf/2004.05150.pdf) this is expected because the model didn't learn yet to work with the sliding window attention.\n",
        "\n",
        "- MLM validation `bpc` after pretraining for a few number of steps: **2.628**. It is quickly getting better. By 3k steps, it should be better than the **2.536** of `roberta-base`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GOc0_6SbD6u"
      },
      "source": [
        "5) Copy global projection layers. MLM pretraining doesn't train global projections, so we need to call `copy_proj_layers` to copy the local projection layers to the global ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "obupoA0FbD6v"
      },
      "outputs": [],
      "source": [
        "logger.info(f'Copying local projection layers into global projection layers ... ')\n",
        "model = copy_proj_layers(model)\n",
        "logger.info(f'Saving model to {model_path}')\n",
        "model.save_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "NOBmcHTj3NPz"
      },
      "outputs": [],
      "source": [
        "logger.info(f'Loading the model from {model_path}')\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(model_path)\n",
        "model = RobertaLongForMaskedLM.from_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I50JkSLXDRNS",
        "outputId": "77481197-ac6f-4115-ce0f-82a832ecbe69"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaLongForMaskedLM(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(32000, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaLongSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): RobertaLMHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (decoder): Linear(in_features=768, out_features=32000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "longformer = LongformerForMaskedLM.from_pretrained('allenai/longformer-base-4096')\n",
        "longformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dECyeERME63u",
        "outputId": "49e8ea9f-1224-402e-aadd-9c6ec2af4806"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LongformerForMaskedLM(\n",
              "  (longformer): LongformerModel(\n",
              "    (embeddings): LongformerEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
              "    )\n",
              "    (encoder): LongformerEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x LongformerLayer(\n",
              "          (attention): LongformerAttention(\n",
              "            (self): LongformerSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (output): LongformerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): LongformerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): LongformerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): LongformerLMHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def to_longformer(model):\n",
        "    new_model = LongformerForMaskedLM.from_pretrained('allenai/longformer-base-4096')\n",
        "    new_model.longformer.embeddings.word_embeddings = model.roberta.embeddings.word_embeddings\n",
        "    new_model.longformer.embeddings.token_type_embeddings = model.roberta.embeddings.token_type_embeddings\n",
        "    new_model.longformer.embeddings.LayerNorm = model.roberta.embeddings.LayerNorm\n",
        "    new_model.longformer.embeddings.position_embeddings = model.roberta.embeddings.position_embeddings\n",
        "    for i, layer in enumerate(new_model.longformer.encoder.layer):\n",
        "        layer.attention.self.query = model.roberta.encoder.layer[i].attention.self.query\n",
        "        layer.attention.self.query_global = model.roberta.encoder.layer[i].attention.self.query_global\n",
        "        layer.attention.self.key = model.roberta.encoder.layer[i].attention.self.key\n",
        "        layer.attention.self.key_global = model.roberta.encoder.layer[i].attention.self.key_global\n",
        "        layer.attention.self.value = model.roberta.encoder.layer[i].attention.self.value\n",
        "        layer.attention.self.value_global = model.roberta.encoder.layer[i].attention.self.value_global\n",
        "        layer.attention.output.dense = model.roberta.encoder.layer[i].attention.output.dense\n",
        "        layer.attention.output.LayerNorm = model.roberta.encoder.layer[i].attention.output.LayerNorm\n",
        "        layer.intermediate.dense = model.roberta.encoder.layer[i].intermediate.dense\n",
        "        layer.output.dense = model.roberta.encoder.layer[i].output.dense\n",
        "        layer.output.LayerNorm = model.roberta.encoder.layer[i].output.LayerNorm\n",
        "    new_model.config.vocab_size = model.config.vocab_size\n",
        "    new_model.config.hidden_size = model.config.hidden_size\n",
        "    new_model.config.num_hidden_layers = model.config.num_hidden_layers\n",
        "    new_model.config.num_attention_heads = model.config.num_attention_heads\n",
        "    new_model.config.type_vocab_size = model.config.type_vocab_size\n",
        "    new_model.config.max_position_embeddings = model.config.max_position_embeddings\n",
        "    new_model.config.attention_window = model.config.attention_window\n",
        "\n",
        "    new_model.lm_head.dense = model.lm_head.dense\n",
        "    new_model.lm_head.layer_norm = model.lm_head.layer_norm\n",
        "    new_model.lm_head.decoder = model.lm_head.decoder\n",
        "    return new_model"
      ],
      "metadata": {
        "id": "8s8D67C3DF5n"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "longfomer_model = to_longformer(model)\n",
        "longfomer_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jD_rsDGBEcxt",
        "outputId": "8e44425e-7dc6-41da-d995-52f52a2356cb"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LongformerForMaskedLM(\n",
              "  (longformer): LongformerModel(\n",
              "    (embeddings): LongformerEmbeddings(\n",
              "      (word_embeddings): Embedding(32000, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
              "    )\n",
              "    (encoder): LongformerEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x LongformerLayer(\n",
              "          (attention): LongformerAttention(\n",
              "            (self): LongformerSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (output): LongformerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): LongformerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): LongformerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): LongformerLMHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (decoder): Linear(in_features=768, out_features=32000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Dataset.from_pandas(pd.read_csv(training_args.train_datapath))\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxVPNEJ2GhjO",
        "outputId": "858434d9-72a3-4564-eaaa-5e22f1e480af"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text'],\n",
              "    num_rows: 18720\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)"
      ],
      "metadata": {
        "id": "v-BNZltgKZVh"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(lambda examples: tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True), batched=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "013ba76d825041a989ee4cfbe4f09d80",
            "178c9df48c544535b6da5e93ffab8993",
            "c7bb44e43893441cba333d2d4aaafe2f",
            "8ab5538b6dc44a7283989ff83edf2496",
            "e5fe8baf6e71457184952b943de3ef80",
            "191339c9063f48d5879332cac78a3bd3",
            "7ad22c3cd82b4b7398c924f95bec78fb",
            "f682b979cb7241e6a8b815b215301015",
            "ab117828b2ed41ba89cb5fffd8f7caf5",
            "2c75134c1db745d8a08d56dc2d1d6a3c",
            "7c86deaf12be45c28c2227b8814112a0"
          ]
        },
        "id": "Ix4cR1BjKn26",
        "outputId": "f6c0a6ff-848d-44e7-eb0b-4bfe24325919"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/18720 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "013ba76d825041a989ee4cfbe4f09d80"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=longfomer_model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynlb_5jYKw3O",
        "outputId": "e775337a-739c-4fc5-c65d-480d6e5382f9"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        },
        "id": "ZwvIIIO6Isyn",
        "outputId": "fb20a7f5-ac01-45ce-f476-63ffd23e38d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [   4/3000 03:03 < 76:14:43, 0.01 it/s, Epoch 0.01/11]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(f'{training_args.output_dir}/longformer-base-4096')"
      ],
      "metadata": {
        "id": "8uR7YQwgNoBQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "013ba76d825041a989ee4cfbe4f09d80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_178c9df48c544535b6da5e93ffab8993",
              "IPY_MODEL_c7bb44e43893441cba333d2d4aaafe2f",
              "IPY_MODEL_8ab5538b6dc44a7283989ff83edf2496"
            ],
            "layout": "IPY_MODEL_e5fe8baf6e71457184952b943de3ef80"
          }
        },
        "178c9df48c544535b6da5e93ffab8993": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_191339c9063f48d5879332cac78a3bd3",
            "placeholder": "​",
            "style": "IPY_MODEL_7ad22c3cd82b4b7398c924f95bec78fb",
            "value": "Map: 100%"
          }
        },
        "c7bb44e43893441cba333d2d4aaafe2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f682b979cb7241e6a8b815b215301015",
            "max": 18720,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ab117828b2ed41ba89cb5fffd8f7caf5",
            "value": 18720
          }
        },
        "8ab5538b6dc44a7283989ff83edf2496": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c75134c1db745d8a08d56dc2d1d6a3c",
            "placeholder": "​",
            "style": "IPY_MODEL_7c86deaf12be45c28c2227b8814112a0",
            "value": " 18720/18720 [00:28&lt;00:00, 646.64 examples/s]"
          }
        },
        "e5fe8baf6e71457184952b943de3ef80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "191339c9063f48d5879332cac78a3bd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ad22c3cd82b4b7398c924f95bec78fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f682b979cb7241e6a8b815b215301015": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab117828b2ed41ba89cb5fffd8f7caf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2c75134c1db745d8a08d56dc2d1d6a3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c86deaf12be45c28c2227b8814112a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}